# WebFunctions library

## Description

This code is like a Swiss Army knife for text. It's designed to grab text from all sorts of places â€“ websites, YouTube videos, and even PDF documents. Think of it as a digital librarian that can go out, find information, and bring it back in a clean, readable format. It also has a built-in safety feature to check if website addresses (URLs) might be dangerous by looking them up in a list of known bad actors.

To do all this, the code uses a bunch of helpful tools, like special instructions for talking to websites, understanding the structure of web pages, and even getting information from YouTube. It also has its own custom tools for handling files and making sure other tools work smoothly. For instance, there's a function to fix text that might be displayed strangely, like when special characters don't show up right.

The code is particularly good at handling YouTube content. It can pull out the descriptive tags associated with a video, giving you a quick summary of what it's about. It can also fetch the entire spoken content of a YouTube video, like a transcript, and present it as plain text. For PDF documents, it's like a scanner that reads through the pages and extracts all the written words.

When it comes to regular websites, the code is quite clever. It can try to get the content directly, or if that doesn't work, it can use a special service to fetch the information. It's also smart enough to recognize if a webpage is actually a PDF and will process it accordingly. Before giving you the text, it cleans up the web page by removing things like navigation menus, advertisements, and other elements that aren't part of the main content, leaving you with just the important words.

Finally, the code includes a set of tools to help identify potentially unsafe web links. It can find all the links within a piece of text, figure out the address (domain) of a website, and even check if that website's address has been flagged for suspicious activity by looking it up in a database. This helps protect users from potentially harmful online content.

## Technicals

The provided Python code is a multifaceted toolkit designed for text extraction, processing, and URL safety analysis. It begins with standard Python imports such as `sys`, `os`, `io`, `copy`, `itertools`, `functools`, `inspect`, `traceback`, `datetime`, `time`, `random`, `json`, `string`, `re`, and `requests`. Additionally, it imports specialized libraries like `pdfplumber` for PDF processing, `youtube_transcript_api` for YouTube transcripts, `scrapingant_client` aliased as `Ant` for web scraping, and `playwright.sync_api` for browser automation. Custom modules `DecoratorFunctions` (aliased as `DF`), `CoreFunctions` (aliased as `CF`), and `FileFunctions` (aliased as `FF`) are also imported, suggesting the use of custom decorators and utility functions.

The `DecodeHashCodes` function, decorated with `@DF.function_trapper(None)`, takes a single parameter `input_string`. Its purpose is to decode numeric character references, such as `&#65;`, within the input string. It employs a regular expression `r'&#(\d+);'` to find these entities. A nested function, `replace_entity`, is defined to handle the replacement. This nested function extracts the numeric code from the matched group, attempts to convert it to an integer, and then uses the `chr()` function to convert this integer into its corresponding character. If a `ValueError` occurs during the integer conversion (indicating an invalid numeric entity), the original matched string is returned. The `re.sub()` function is used to apply this replacement across the entire `input_string`, returning the decoded string.

The `yttags2text` function, decorated with `@DF.function_trapper('{[(*VNF*)]}')`, accepts two parameters: `url` and an optional `userhome` which defaults to `None`. This function is designed to extract tags from a YouTube video. It includes an inner helper function `extract_video_id` that uses a regular expression `r'(?:youtu\.be\/|youtube\.com\/(?:.*v=|.*\/|.*v\/|.*embed\/|.*shorts\/))([a-zA-Z0-9_-]{11})'` to parse various YouTube URL formats and extract the 11-character video ID. Another inner function, `get_video_tags`, takes a `video_id` and utilizes the YouTube Data API (via a `youtube` object built using `build('youtube', 'v3', developerKey=Tokens['YouTube'])`) to fetch video details. It specifically requests the 'snippet' part, which contains tags. If the video is found and has tags, they are returned as a list; otherwise, `None` or a placeholder `'{[(*VNF*)]}'` is returned. The main function first reads tokens using `FF.ReadTokens(userhome=userhome)`, then extracts the video ID, fetches the tags, and if the tags are a list, it joins them into a single string separated by newlines before returning.

The `youtube2text` function, decorated with `@DF.function_trapper(None)`, takes a `video_url` and an optional `retry` parameter, defaulting to 3. This function's primary goal is to retrieve the transcript of a YouTube video. It begins by extracting the video ID from the `video_url` using the regular expression `r'(v=|be/|embed/|v/|youtu\.be/|\/videos\/|\/shorts\/|\/watch\?v=|\/watch\?si=|\/watch\?.*?&v=)([a-zA-Z0-9_-]{11})'`, capturing the 11-character ID from the second group. It then enters a `while` loop that iterates up to the `retry` count. Inside the loop, it initializes `youtube_transcript_api.YouTubeTranscriptApi()`, calls its `fetch(video_id)` method to get the transcript, and if successful, breaks the loop by setting `c` to a value greater than `retry`. If an exception occurs during fetching, it prints an error message and waits for 3 seconds before retrying. After the loop, if `transcript_list` is empty, it returns `None`. Otherwise, it iterates through the `transcript_list` (which contains dictionaries with 'text', 'start', and 'duration' keys), concatenates the 'text' from each snippet into `transcript_text` with a newline, and returns this string prefixed with "Video Transcript: ".

The `PDF2Text` function, decorated with `@DF.function_trapper(None)`, accepts a single parameter `pdf_buffer`, which is expected to be the byte content of a PDF file. It utilizes `pdfplumber.open(io.BytesIO(pdf_buffer))` to open the PDF from the provided buffer. It then iterates through each `page` in `pdf.pages`, extracts the text from each page using `page.extract_text()`, and concatenates it into a single string variable `text`. Finally, it returns this accumulated text prefixed with "PDF Content: ".

The `ScrapingAnt` function, decorated with `@DF.function_trapper(None)`, takes two parameters: `url` (the URL to scrape) and an optional `userhome` (defaulting to `None`). This function interacts with the ScrapingAnt API. It first reads API tokens using `FF.ReadTokens(userhome=userhome)`. It then attempts to create an instance of `Ant.ScrapingAntClient` using the 'ScrapingAnt' token from the read tokens. Subsequently, it calls the `client.general_request(url)` method to fetch content from the specified URL. If any exception occurs during this process (e.g., network issues, invalid token, API errors), the `except` block catches it and the function returns `None`. If the request is successful, it returns the `content` attribute of the `result` object.

The `StripHTML` function takes a single parameter `htmlbuf`, which is a string containing HTML content. This function is designed to clean HTML by removing specific tags and normalizing whitespace. It uses `re.sub()` with the `re.DOTALL` flag to ensure that multiline content within tags is also matched and removed. First, it removes the entire `<head>` section. Then, it removes `<script>` and `<style>` elements. After that, it specifically targets `<a>` tags, replacing them with their inner content (`r'\1'`) to strip away the hyperlink structure while retaining the link text. Subsequently, it removes all remaining HTML tags using `r'<[^>]+>'`. Finally, it collapses multiple whitespace characters into single spaces using `re.sub(r'\s+', ' ', text)` and removes leading/trailing whitespace with `.strip()`, returning the cleaned text.

The `html2text` function, which is not explicitly decorated with a `function_trapper` in the provided snippet but is described as a versatile tool, takes four parameters: `url`, `external` (boolean, defaults to `False`), `userhome` (defaults to `None`), and `raw` (boolean, defaults to `False`). This function aims to extract text content from a given URL, handling various content types. It first checks if the URL is a YouTube watch or short URL. If so, it calls `youtube2text(url)` and returns the result. Otherwise, it proceeds to fetch the HTML content. If `external` is `True`, it calls `ScrapingAnt(url, userhome=userhome)`. If `external` is `False`, it attempts to fetch the content using Playwright. This involves launching a Chromium browser, creating a new context with a specified `userAgent`, creating a new page, setting a default timeout of 60 seconds, navigating to the `url`, and then retrieving the page content using `page.content()`. The browser is then closed. If any error occurs during fetching (either external or internal), `html` is set to `None`.

After fetching, it checks if the `html` content starts with the PDF signature `'%PDF-'`. If it does, and if `html` is not already bytes, it's encoded to UTF-8. Then, `PDF2Text(html)` is called, and the result is stripped and returned. If the content is not a PDF, and if `html` is of type `bytes`, it's decoded using `html.decode('utf-8', errors='ignore')` and then passed to `DecodeHashCodes`. If the `raw` parameter is `True`, the (potentially decoded) `html` string is returned directly without further processing. Otherwise, the `html` is parsed using `BeautifulSoup(html, 'html.parser')`. Script and style tags are removed using `tag.decompose()`. The remaining text is extracted using `soup.get_text().strip()`. If the extracted `text` is empty, `None` is returned. Otherwise, the text is normalized by stripping whitespace from each line, removing empty lines, and joining the remaining lines with a single newline. This cleaned text is then returned, prefixed with "Web Page Content: ".

The code also includes a section for URL safety analysis, starting with the `ExtractURLs` function, decorated with `@DF.function_trapper`. This function takes a single parameter `text` and uses a compiled regular expression `r'https?://[^\s]+'` to find all occurrences of URLs (starting with http or https and followed by non-whitespace characters) within the input `text`. It returns a list of all found URLs.

The `Domain2IP` function, decorated with `@DF.function_trapper`, takes a `domain` name as input. It attempts to resolve the domain to an IP address using `socket.gethostbyname(domain)`. If successful, it returns the IP address. If any exception occurs during this process (e.g., `socket.gaierror` for DNS resolution failure), the exception is caught, but no specific action is taken other than passing, and the function returns `None`.

The `ExtractDomains` function, decorated with `@DF.function_trapper`, takes a `url` as input. It uses `urlparse(url)` from the `urllib.parse` module to parse the URL into its components. It then returns the `netloc` attribute of the parsed URL, which represents the network location (domain name and optional port).

The `CheckAbuseIPDB` function, decorated with `@DF.function_trapper`, takes a `domain` and an optional `userhome` parameter. It first calls `Domain2IP(domain)` to get the IP address of the domain. If `Domain2IP` returns `None`, this function also returns `None, 0`. Otherwise, it reads tokens using `FF.ReadTokens(userhome=userhome)`. It then constructs a URL for the AbuseIPDB API's `/check` endpoint and sets up `params` with the `ipAddress` and `headers` including a `User-Agent`, the AbuseIPDB API `Key` from the tokens, and an `Accept` header. It makes a GET request to the AbuseIPDB API using `requests.get()` with a timeout of 60 seconds. `response.raise_for_status()` is called to raise an exception for bad status codes. If the request is successful, the JSON response is parsed. It checks for the presence and value of `data.get('data', {}).get('abuseConfidenceScore', 0)`. If this score is greater than 0, it returns `True` and the score. Otherwise, it returns `False` and `0`. If a `requests.exceptions.RequestException` occurs during the API call, it prints an error message and returns `None, 0`.
